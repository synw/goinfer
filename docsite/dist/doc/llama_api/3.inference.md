# Inference queries

## Run inference

Run an inference request

- `/completion` *POST*: payload:   
  - `prompt` *string* **required**: the prompt text
  - `template` *string*: the template to use, default *{prompt}*
  - `stream`: *bool*: stream the response if true, default *false*
  - `model`: the model to use and it's params, uses the currently loaded in memory model if not provided:
    - `name`: *string*: name of the model to load
    - `ctx`: *int*: the context window size to use with the model
  - `ctx` *int*: the context window size to use with the model
  - `threads` *int*: the number of threads to use, default *4*
  - `max_tokens` *int*: the maximum number of tokens to predict, default *512*
  - `top_k` *int*: the top K sampling param (limit the next token selection to the K most probable tokens), default *40*
  - `top_p` *int*: the top P sampling param (limit the next token selection to a subset of tokens with a cumulative probability above a threshold P), default *0.95*
  - `temperature` *int*: the temperature param (randomness of the generated text), default *0.2*
  - `frequency_penalty` *int*: the frequency penalty param , default *0*
  - `presence_penalty` *int*: the presence penalty param , default *0*
  - `repeat_penalty` *int*: the repeat penalty param , default *1.0*
  - `tfs` *float64*: the tail free sampling param (to reduce the probabilities of less likely tokens to appear), default *1.0* (disabled)
  - `stop` *[]string*: the stop tokens param to stop inference if met, default *[]*
  
Example post payload:

```js
{
  "prompt": "List the planets in the solar system",
  "template": "### Instruction: {prompt}\n\n### Response:",
  "temperature": 0.2,
  "top_p": 0.8
}
```

A `{prompt}` slot is available for custom templates: this is where the prompt text
will be placed

## Abort inference

Abort a running inference

- `/comletion/abort` *GET*: will return a `204` status code if the inference was aborted, and a `202` in case of nothing to abort

## Example

```bash
curl -X POST -H "Content-Type: application/json" -d \
'{"prompt": "List the planets in the solar system", "template": \
"### Instruction: {prompt}\n### Response:"}' \
http://localhost:5143/completion  | python -m json.tool
```

Response:

```js
{
  "num": 57,
  "content": "result",
  "msg_type": "system",
  "data": {
    "content": " \n Sure, here are the planets in our solar system in order from the sun:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune",
    "thinkingTime": 0.759748491,
    "thinkingTimeFormat": "759.748491ms",
    "emitTime": 8.728113106,
    "emitTimeFormat": "8.728113106s",
    "totalTime": 9.487861597,
    "totalTimeFormat": "9.487861597s",
    "tokensPerSecond": 6.42,
    "totalTokens": 56,
  }
}
```