# Start the inference engine

The `llama-server` loads a model:

- `/model/start` *POST*: payload:
  - `model` *string* **required**: the model name to load
  - `ctx` *int*: context window size, default *2048*

## Example

```bash
curl -X POST 0.0.0.0:5143/model/start -H 'Authorization: Bearer 67b164c5862183d9cce2e185b3e79bca58b915284ec7fc347a08ca69f07e466b' -H "Content-Type: application/json" -d '{"model": "UIGEN-T3-14B-Preview-Q8_0-GGUF_q8_0.gguf"}' | python -m json.tool
```

The JSON message can also be:

```js
{
  "model": "orca-mini-3b.gguf.q8_0.bin"
}
```

or:

```js
{
  "model": "mistral-7b-instruct-v0.1.Q4_K_M.gguf",
  "ctx": 8192
}
```

### Possible response status codes

- `204`: the model has been loaded successfully
- `202`: the requested model is already loaded on the server
- `404`: the requested model has not been found on the server
- `500`: error loading model

*Note*: if the requested model is already loaded it will not reload it and provide a
`202` status code. To reload the model with new parameters use `/model/stop` first
and the load the model again

## Unload a model

To unload a model and stop the LLM engine, use the `/model/stop` *GET* endpoint

```bash
curl 0.0.0.0:5143/model/stop -H 'Authorization: Bearer 67b164c5862183d9cce2e185b3e79bca58b915284ec7fc347a08ca69f07e466b'
```
