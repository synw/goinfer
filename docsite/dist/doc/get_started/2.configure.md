# Configure

Create a config file at the root

## Api server mode

```bash
./goinfer -conf /absolute/path/to/models/directory
```

## Local mode with gui

```bash
./goinfer -localconf /absolute/path/to/models/directory
```

Provide an absolute path to your gguf models directory

This will create a `goinfer.yml` file

### Example

```yaml
models_dir: ./models

server:
	api_key:
		# ‚ö†Ô∏è Set 64-byte secure API keys üö®
		admin: "PLEASE SET SECURE API KEY"
		user:  "PLEASE SET SECURE API KEY"
	origins: "localhost"
	ports:
		admin:   "9999"
		goinfer: "2222"
		mcp:     "3333"
		openai:  "5143"

llama:
	exe_path: ./llama-server
	args:
		// --props: enable changing global properties via POST /props
		// --no-webui: no Web UI server
		common: --props --no-webui --no-warmup
		goinfer: --jinja --chat-template-file template.jinja

```

### Parameters

- `server.api_key`: *string* **required**: the API key to protect some server endpoints
- `server.origins` *[]string*: a list of authorized CORS urls
- `models_dir` *string*: the absolute path to the models directory
- TODO: complete
