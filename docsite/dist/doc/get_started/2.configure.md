# Configure

Create a config file at the root

## Api server mode

```bash
./goinfer -conf /absolute/path/to/models/directory
```

## Local mode with gui

```bash
./goinfer -localconf /absolute/path/to/models/directory
```

Provide an absolute path to your gguf models directory

This will create a `goinfer.yml` file

### Example

```yaml
llama:
    args:
        - --log-colors
        - --no-warmup
    exe: ./llama-server
    host: localhost
    port: 8080
    t_prompt: 16
    threads: 8
    web_ui: false
model:
    ctx: 2048
    dir: ./models
    flash_attention: true
    gpu_layers: 999
    name: ""
server:
    api_key: 7aea109636aefb984b13f9b6927cd174425a1e05ab5f2e3935ddfeb183099465
    openai_api: false
    origins:
        - http://localhost:5173
        - http://localhost:5143
    port: 5143
```

### Parameters

- `model.dir` *string* **required**: the absolute path to the models directory
- `sever.api_key`: *string* **required**: the api key used for the api server mode (see below)
- `sever.origins` *[]string*: a list of authorized CORS urls