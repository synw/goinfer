# Configure

Create a config file at the root

## Api server mode

```bash
./goinfer -conf /absolute/path/to/models/directory
```

## Local mode with gui

```bash
./goinfer -localconf /absolute/path/to/models/directory
```

Provide an absolute path to your gguf models directory

This will create a `goinfer.yml` file

### Example

```yaml
llama:
    exe_path: ./llama-server
    threads: 8
    t_prompt_proc: 16
    args:
        - --log-colors
        - --no-warmup
models_dir: ./models
server:
    port: 5143
    origins:
        - http://localhost:5173
        - http://localhost:5143
    openai_api: false
    api_key: 8a2f480d2c62216fdde5e06c88b0539d6a0f7030e051c434df321aafcfc7ff0d
```

### Parameters

- `server.api_key`: *string* **required**: the API key to protect some server endpoints
- `server.origins` *[]string*: a list of authorized CORS urls
- `models_dir` *string*: the absolute path to the models directory
- TODO: complete
