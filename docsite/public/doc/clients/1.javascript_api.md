# Javascript api

Install the [@goinfer/api](https://github.com/synw/goinfer-js/packages/api) library:

```bash
npm install @goinfer/api
# or
yarn add @goinfer/api
```

## Doc andÂ example

[Api doc](https://synw.github.io/goinfer-js/api/index.html)

[Typescript example usage](https://github.com/synw/goinfer/blob/main/examples/js/template/goinfer_lib.ts)

## Usage

Initialize:

```ts
import { useGoinfer } from "@goinfer/api";

const api = useGoinfer({
  serverUrl: "http://localhost:5143",
  apiKey: "api_key",
});
```

With parameters to control streaming:

```ts
const goinfer = useGoinfer({
  serverUrl: "http://localhost:5143",
  apiKey: "api_key",
  onToken: (token) => {
    console.log("Received token:", token);
  },
  onStartEmit: (stats) => {
    console.log("Emission started:", stats);
  },
  onError: (errorMsg) => {
    console.error("Error encountered:", errorMsg);
  },
});
```

Types:

```ts
interface TempInferStats {
  thinkingTime: number;
  thinkingTimeFormat: string;
}

interface GoinferParams {
  serverUrl: string;
  apiKey: string;
  onToken?: (t: string) => void;
  onStartEmit?: (s: TempInferStats) => void;
  onError?: (e: string) => void;
}
```

### Models state

Get the current models state from the server with info about the template type and context window size:

```ts
import type { ModelState } from "@goinfer/api";

const modelsState: ModelState = await api.modelsState();
```

Example response:

```javascript
{
  "ctx": 1024,
  "isModelLoaded": false,
  "loadedModel": "",
  "models": {
    'WizardVicuna-Uncensored-3B-0719.gguf.q8_0.bin': { name: 'wizard_vicuna', ctx: 2048 },
    'llama-2-7b-chat-codeCherryPop.gguf.q4_K_M.bin': { name: 'alpaca', ctx: 4096 },
    'orca-mini-3b.gguf.q8_0.bin': { name: 'orca', ctx: 2048 },
  }
}
```

Types:

```ts
interface ModelTemplate {
  name: string;
  ctx: number;
}

interface ModelState {
  models: Record<string, ModelTemplate>;
  isModelLoaded: boolean;
  loadedModel: string;
  ctx: number;
}
```

### Model Loading and Unloading

To load a model for inference:

```ts
await api.loadModel({
  name: "orca-mini-3b.gguf.q8_0.bin",
  ctx: 2048
});
```

To unload the currently loaded model:

```ts
await api.unloadModel();
```

### Inference

Run an inference query using the loaded model and default params:

```ts
const result = await api.infer("Prompt here");
console.log(result.text);
```

Run an inference query providing a model and some inference params:

```ts
const inferParams: InferParams = {
  streaming: true,
  temperature: 0.2,
  top_p: 0.35,
  max_tokens: 250,
};
const result = await api.infer("Prompt here", inferenceParams);
console.log(result.text);
```

Types:

```ts
interface ModelConf {
  name: string,
  ctx: number,
  rope_freq_scale?: number,
  rope_freq_base?: number,
}

interface InferParams {
  stream?: boolean;
  model?: ModelConf;
  threads?: number;
  max_tokens?: number;
  top_k?: number;
  top_p?: number;
  temperature?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  repeat_penalty?: number;
  tfs?: number;
  stop?: Array<string>;
}

interface TempInferStats {
  thinkingTime: number;
  thinkingTimeFormat: string;
}

interface InferResult extends TempInferStats {
  text: string;
  inferenceTime: number;
  emitTime: number;
  emitTimeFormat: string;
  totalTime: number;
  totalTimeFormat: string;
  tokensPerSecond: number;
  totalTokens: number;
}
```

### Aborting Inference

To halt any running inference process:

```ts
await api.abort();
```

### Tasks

Get a list of available tasks:

```ts
const tasks = await api.loadTasks();
```

Fetch details of a particular task:

```ts
const taskDetails = await api.loadTask("code/json/fix");
```

To run a task:

```ts
await api.executeTask("code/json/fix", "Prompt here");
```

Types:

```ts
interface Task {
  name: string;
  template: string;
  modelConf: ModelConf;
  inferParams: InferParams,
}
```


