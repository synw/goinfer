# - Requirement:  local file ./goinfer.yml
# - Build:        DOCKER_BUILDKIT=1 docker compose build
# - Run:          docker compose up
# - Stop:         CTRL + C
# - Clean:        docker compose down

services:
  goinfer-llama:
    image: goinfer-llama
    container_name: goinfer-llama
    build:
      context: .
      dockerfile: ./Dockerfile
      args:
        # llama.cpp tags: https://github.com/ggml-org/llama.cpp/tags
        - llama_git_tag=b6131
        # Nvidia container images: https://hub.docker.com/r/nvidia/cuda
        - CUDA_VERSION=13.0
        - CUDA_PATCH=0
        - UBUNTU_VERSION=24.04
    command: -local
    ports: [5143:5143]
    configs: [goinfer.yml]
    volumes:
      # MODELS_DIR is also used by goinfer
      - ${MODELS_DIR:-./models}:/models
      - ${CCACHE_DIR:-./.ccache}:/root/.ccache
      - ${GO_CACHE_DIR:-./.go}:/root/go
    environment:
      - TZ=UTC0 # time.Unix() uses UTC instead of local time zone
    # restart: unless-stopped

configs:
  goinfer.yml:
    # file: ./goinfer.yml
    content: 'server: {"api_key": "fe4efada8bb2c8daef1acf3c9a847ffea767376ee9e3c109d1225075833e3766"}'
