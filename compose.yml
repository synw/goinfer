# - Requirement:  local file ./goinfer.yml
# - Build:        DOCKER_BUILDKIT=1 docker compose build
# - Run:          docker compose up
# - Stop:         CTRL + C
# - Clean:        docker compose down

services:
  goinfer-llama:
    image: goinfer-llama
    container_name: goinfer-llama
    build:
      context: .
      dockerfile: ./Dockerfile
      args:
        # llama.cpp tags: https://github.com/ggml-org/llama.cpp/tags
        - llama_git_tag=b6131
        # Nvidia container images: https://hub.docker.com/r/nvidia/cuda
        - CUDA_VERSION=13.0
        - CUDA_PATCH=0
        - UBUNTU_VERSION=24.04
    command: -local
    ports: [5143:5143]
    volumes:
      # MODELS_DIR is also used by goinfer
      - ${MODELS_DIR:-./models}:/models
      - ${CCACHE_DIR:-./.ccache}:/root/.ccache
      - ${GO_CACHE_DIR:-./.go}:/root/go
      - ./goinfer.yml:/app/goinfer.yml
    environment:
      - TZ=UTC0 # time.Unix() uses UTC instead of local time zone
    # restart: unless-stopped
